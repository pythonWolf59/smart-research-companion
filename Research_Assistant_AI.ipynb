{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNER1oCK4N2XtHHF1w0pCmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pythonWolf59/smart-research-companion/blob/dev/Research_Assistant_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "raGSuMkxlZ5p"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install openai -q  gradio PyMuPDF feedparser fpdf duckduckgo_search arxiv requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "from arxiv import Search as ArxivSearch, SortCriterion\n",
        "from fpdf import FPDF\n",
        "import arxiv\n",
        "import requests\n",
        "\n",
        "# --- Get Groq API key ---\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=groq_api_key\n",
        ")\n",
        "\n",
        "# --- PDF Text Extraction ---\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    with open(pdf_file.name, \"rb\") as f:\n",
        "        doc = fitz.open(stream=f.read(), filetype=\"pdf\")\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# --- Chunk and Summarize ---\n",
        "def chunk_text(text, max_tokens=3000):\n",
        "    paragraphs = text.split(\"\\n\")\n",
        "    chunks, chunk = [], \"\"\n",
        "    for para in paragraphs:\n",
        "        if len(chunk) + len(para) < max_tokens:\n",
        "            chunk += para + \"\\n\"\n",
        "        else:\n",
        "            chunks.append(chunk)\n",
        "            chunk = para + \"\\n\"\n",
        "    if chunk:\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# Summarize with AI\n",
        "\n",
        "def summarize_with_groq(text):\n",
        "    chunks = chunk_text(text)\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        \"You are an AI assistant helping with academic research.\\n\"\n",
        "                        \"Summarize the following research abstract in a clean, readable format with:\\n\"\n",
        "                        \"- **Bold section titles** like Introduction, Method, Results (if applicable)\\n\"\n",
        "                        \"- Bullet points for key insights\\n\"\n",
        "                        \"- Paragraph breaks\\n\"\n",
        "                        \"- Markdown format output\\n\\n\"\n",
        "                        f\"{chunk}\"\n",
        "                    )\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        summaries.append(response.choices[0].message.content.strip())\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(summaries)\n",
        "\n",
        "# Search Papers\n",
        "\n",
        "# --- Multi-source search & caching ---\n",
        "cached_abstracts = {}   # URL ‚Üí abstract\n",
        "title_to_url = {}       # Title ‚Üí URL\n",
        "\n",
        "def search_all_sources(query):\n",
        "    global cached_abstracts, title_to_url\n",
        "    cached_abstracts = {}\n",
        "    title_to_url = {}\n",
        "    lines = []\n",
        "\n",
        "    # arXiv\n",
        "    arxiv_search = ArxivSearch(query=query, max_results=25, sort_by=SortCriterion.Relevance)\n",
        "    arxiv_client = ArxivSearch.client if hasattr(ArxivSearch, \"client\") else None\n",
        "    for res in ArxivSearch(query=query, max_results=25, sort_by=SortCriterion.Relevance).results():\n",
        "        title, url, abs = res.title.strip(), res.entry_id, res.summary.strip()\n",
        "        lines.append(f\"üîó [arXiv] <a href='{url}' target='_blank'>{title}</a>\")\n",
        "        title_to_url[title] = url\n",
        "        cached_abstracts[url] = abs\n",
        "\n",
        "    # Semantic Scholar\n",
        "    ss = requests.get(\n",
        "        \"https://api.semanticscholar.org/graph/v1/paper/search\",\n",
        "        params={\"query\": query, \"limit\": 25, \"fields\": \"title,url,abstract\"}\n",
        "    ).json().get(\"data\", [])\n",
        "    for p in ss:\n",
        "        title, url, abs = p.get(\"title\"), p.get(\"url\"), p.get(\"abstract\", \"\")\n",
        "        if title and url:\n",
        "            lines.append(f\"üîó [SemScholar] <a href='{url}' target='_blank'>{title}</a>\")\n",
        "            title_to_url[title] = url\n",
        "            cached_abstracts[url] = abs\n",
        "\n",
        "    # CORE\n",
        "    core = requests.get(\n",
        "        \"https://api.core.ac.uk/v3/search/works\",\n",
        "        params={\"q\": query, \"limit\": 25}\n",
        "    ).json().get(\"data\", [])\n",
        "    for p in core:\n",
        "        title, url, abs = p.get(\"title\"), p.get(\"id\"), p.get(\"abstract\", \"\")\n",
        "        if title and url:\n",
        "            lines.append(f\"üîó [CORE] <a href='{url}' target='_blank'>{title}</a>\")\n",
        "            title_to_url[title] = url\n",
        "            cached_abstracts[url] = abs\n",
        "\n",
        "    # PubMed\n",
        "    pm = requests.get(\n",
        "        \"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pmc/?format=json&tool=mytool&email=me@example.com&term=\" + query\n",
        "    ).json().get(\"records\", [])[:25]\n",
        "    for p in pm:\n",
        "        title = p.get(\"title\")\n",
        "        url = p.get(\"pmcid\") and f\"https://www.ncbi.nlm.nih.gov/pmc/{p['pmcid']}\"\n",
        "        abs = p.get(\"abstractText\", \"\")\n",
        "        if title and url:\n",
        "            lines.append(f\"üîó [PubMed] <a href='{url}' target='_blank'>{title}</a>\")\n",
        "            title_to_url[title] = url\n",
        "            cached_abstracts[url] = abs\n",
        "\n",
        "    return \"\\n\".join(lines), gr.update(choices=list(title_to_url.keys()), value=None)\n",
        "\n",
        "def fetch_summary(title):\n",
        "    global title_to_url, cached_abstracts\n",
        "    url = title_to_url.get(title)\n",
        "    abs_text = cached_abstracts.get(url, \"\")\n",
        "    if not abs_text:\n",
        "        return \"No abstract found for this paper.\"\n",
        "    return summarize_with_groq(abs_text)\n",
        "\n",
        "# --- PDF Summarization ---\n",
        "def handle_pdf(pdf_file): return summarize_with_groq(extract_text_from_pdf(pdf_file))\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "def handle_pdf(pdf_file):\n",
        "    text = extract_text_from_pdf(pdf_file)\n",
        "    return summarize_with_groq(text)\n",
        "\n",
        "def launch_app():\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# üß† Research Assistant AI\\nUpload PDF or Search Research Papers\")\n",
        "\n",
        "        with gr.Tab(\"üìÑ Upload PDF\"):\n",
        "            with gr.Row():\n",
        "                pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "                summarize_btn = gr.Button(\"Summarize\")\n",
        "            summary_output = gr.Textbox(label=\"Summary\", lines=20)\n",
        "\n",
        "        with gr.Tab(\"üîç Search Papers\"):\n",
        "            with gr.Row():\n",
        "                prompt_input = gr.Textbox(label=\"Enter your research topic\")\n",
        "                search_btn = gr.Button(\"Search\")\n",
        "            paper_list = gr.HTML(label=\"Search Results\")\n",
        "            paper_dropdown = gr.Dropdown(label=\"Select Paper URL to Summarize\", choices=[])\n",
        "            paper_summary = gr.Textbox(label=\"Paper Summary\", lines=15)\n",
        "\n",
        "        summarize_btn.click(fn=handle_pdf, inputs=pdf_input, outputs=summary_output)\n",
        "        search_btn.click(fn=search_all_sources, inputs=prompt_input, outputs=[paper_list, paper_dropdown])\n",
        "        paper_dropdown.change(fn=fetch_summary, inputs=paper_dropdown, outputs=paper_summary)\n",
        "\n",
        "    demo.launch(debug=True)\n",
        "\n",
        "launch_app()"
      ],
      "metadata": {
        "id": "kPdtM_NP0ntJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
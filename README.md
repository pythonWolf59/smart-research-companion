# ğŸ§  Local AI Research Assistant (FastAPI + Ollama + ChromaDB)

This project is a **fully local, offline-compatible AI research assistant** that allows you to:
- Upload and summarize research papers (PDF)
- Ask questions using Retrieval-Augmented Generation (RAG)
- Store document vectors in **ChromaDB**
- Run local LLMs like Mistral or Gemma using **Ollama**
- Serve via **FastAPI**
- (Optional) Interface with **Streamlit** frontend

---

## ğŸ›  Tech Stack

- **FastAPI** â€“ for backend API
- **Ollama** â€“ run LLMs locally (Mistral/Gemma/etc.)
- **ChromaDB** â€“ vector database
- **PyMuPDF** â€“ PDF text extraction
- **LlamaIndex** or **Haystack** â€“ for document-based QA
- **Supabase** â€“ for auth/data (optional)
- **Streamlit** â€“ for optional frontend

---

## ğŸ“¦ Project Structure

```bash
ai_research_assistant/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ models/
â”œâ”€â”€ rag/
â”œâ”€â”€ frontend/
â”œâ”€â”€ ollama/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

# ğŸš€ Getting Started

1.  **Install Ollama & a Model**

    ```bash
    curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh
    ollama run mistral
    ```

2.  **Install Python Dependencies**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Run FastAPI App**

    ```bash
    uvicorn app.main:app --reload
    ```

4.  **(Optional) Run Streamlit UI**

    ```bash
    streamlit run frontend/streamlit_app.py
    ```

# ğŸ§ª Features

* Local PDF summarization
* Vector store with ChromaDB
* FastAPI endpoints
* Local LLM integration
* Supabase user management
* Semantic search across docs
* Frontend dashboard (optional)

# ğŸ’¡ Future Ideas

* Multi-file indexing
* Paper-to-slide converter
* Citation auto-extraction
* Academic question-answering bot

# âš–ï¸ License

MIT â€” free to use and extend.
